{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例分析一：Kaggle Competition | Titanic Machine Learning from Disaster\n",
    "\n",
    "\n",
    "***本作基于： [Titanic](https://github.com/agconti/kaggle-titanic). 原数据来源： [Kaggle.com](http://www.kaggle.com/c/titanic-gettingStarted).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `import … as …`是给我们引入的包加上缩写，在后续调用的时候可以直接用缩写调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df #直接打变量名字就可以预览数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Ticket','Cabin'], axis=1)\n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.drop()`是用于删除整一列数据，不作额外判断。  \n",
    "* 如果将`axis`参数改为1 (默认为0)，则会沿纵向删除数据  \n",
    "#### `.dropna()`用于删除空值NaN  \n",
    "* 在不额外调整`axis`参数值时，同样默认遍历全部行进行删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 案例分析二： Data Cleaning with NumPy and Pandas\n",
    "  \n",
    "***本作基于： [Data-cleaning](https://github.com/mramshaw/Data-Cleaning). 原数据来源： [Pandas and NumPy](https://realpython.com/python-data-cleaning-numpy-pandas/).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BL-Flickr-Images-Book.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df.head()`意为预览前五列数据\n",
    "* 括号中定义数字可以预览更多行的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Edition Statement',\n",
    "          'Corporate Author',\n",
    "          'Corporate Contributors',\n",
    "          'Former owner',\n",
    "          'Engraver',\n",
    "          'Contributors',\n",
    "          'Issuance type',\n",
    "          'Shelfmarks']\n",
    "df.drop(to_drop, inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df.drop(labels= ，inplace=false，axis=0，errors='raise')`是drop函数的部分语法默认值\n",
    "* labels：在这里用`to_drop`统一标注想要删除的列；在较小的数据集中，可以直接用字符串标明需要删除的列名\n",
    "* axis：默认值为0，按纵向删除；在此改为1，按列方向删除\n",
    "* inplace：默认值为`false`；如果将参数改为`True`，将会直接覆盖原数据，操作不可逆\n",
    "* errors：默认值为`'raise'`，此时如果出现不存在的列名则返回`KeyError`；参数改为'ignore'便可以忽略打错的列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Identifier'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df['Identifier'].is_unique` 是测试某列名是否是唯一的，因为我们想要把在后续以这个列名为基础查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Identifier')\n",
    "# df.set_index('Identifier', inplace=True) 效果一样\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df.set_index('Identifier')` 是将检索值固定为了我们想要的，替换掉了默认的0,1,2…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[216]\n",
    "# df.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df.loc[]` 是查询特定的检索值所在行的数据，在这里是Identifier的编号\n",
    "* 如果loc[]查找的数值不在范围内，会返回'KeyError'\n",
    "#### `df.iloc[]` 是查询相对的检索值所在的数据，默认值是`0`，即第一行的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.get_dtype_counts()--》老版\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df.dtypes.value_counts()` 是查询数据中存在的数据类型\n",
    "* object：是主要关注的结果\n",
    "* Name：在这句代码中基本上可以忽略\n",
    "* dtype：指的是这句代码统计结果的储存方式是整数->在计算比例的时候可能统计结果会出现float64的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1905:, 'Date of Publication'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这样检索“发行日期”的数据，发现格式不是统一的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'^(\\d{4})'\n",
    "extr = df['Date of Publication'].str.extract(r'^(\\d{4})', expand=False)\n",
    "extr.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `r'^(\\d{4})'` 是严格匹配字符串前四位数字的正则表达式\n",
    "* `r' '`表示原始字符串，避免`\\d`被python误识别\n",
    "* `^`:是匹配字符串的起始符\n",
    "* `\\d`：意为匹配任意数字（依赖后续条件）\n",
    "* `{n}`：意为匹配前一个字符n次\n",
    "* `（）`：将匹配的内容捕捉为一个分组，用于后续提取  \n",
    "由于正则表达式，笔者仍掌握不太熟练，其他语法元素可以参考: ***[CDSN论坛](https://blog.csdn.net/weixin_42448623/article/details/102785880)***\n",
    "#### `.str.extract(r'^(\\d{4})', expand=False)`用于从正则表达式中提取匹配的内容\n",
    "* 在正则表达式位置，可以用提前定义好的变量名，在这里可以用`rf^{regex}`来替换原本表达式的参数位置\n",
    "* expand：默认值为`True`，在匹配完数据后返回原数据+匹配后的数据；改为`False`后只返回匹配后的数据，匹配失败的返回`NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date of Publication'] = pd.to_numeric(extr,errors='coerce')\n",
    "df['Date of Publication'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `pd.to_numeric()` 是将目标数据强制转换为数值类型（比如原来是字符串）\n",
    "* `error`：设定参数为`coerce`可以将无效值转换为`NaN`；`ignore`可以保留原始数据类型，但是不建议\n",
    "* 如果全部值都可以转换为整数，则`.dtype`结果会返回`int64`；如果存在NaN，那么会转换为`float64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date of Publication'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们格式化了出版日期的年份。接下来开始格式化出版地名的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Place of Publication'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub = df['Place of Publication']\n",
    "london = pub.str.contains('London')\n",
    "london[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `str.contains()` 是用于检查字符串列是否包含特定子串（可以是正则表达式）\n",
    "* pat：要匹配的是字符串还是正则表达式\n",
    "* case：默认为`True`，意为是否区分大小写\n",
    "* flags：正则表达式的标志（如忽略大小写）\n",
    "* na：处理缺失值的方式，默认将`NaN`处理为`False`\n",
    "* regex：默认为`True`，即视为正则表达式；在这里因为无需匹配额外的符号等，所以用True和False没有区别\n",
    "  \n",
    "在这里结果是：精确匹配是否存在字符串为`\"London\"`  \n",
    "如果改为`.str.contains('London', case=False)`，则可以查到登记为`\"london\"`的出版地\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford = pub.str.contains('Oxford')\n",
    "df['Place of Publication'] = np.where(london, 'London',\n",
    "                                      np.where(oxford, 'Oxford',\n",
    "                                      pub.str.replace('-', ' ')))\n",
    "df['Place of Publication'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `np.where()` 是为了满足：根据不同条件返回不同的值\n",
    "* 参数填充顺序为：  \n",
    "      i.条件1，满足条件时的填充值；  \n",
    "      ii.用嵌套自身的方式填充：条件2，满足条件2时填充的值  \n",
    "      iii.在嵌套的最末尾填上如果不满足时，需要填充的值\n",
    "  \n",
    "* 此处的判断逻辑为：\n",
    "      i.检查在检索的列中，是否存在`London`，满足的话填充\"London\"  \n",
    "      ii.如果没有`London`，转入第二层，检查是否存在`Oxford`，满足的话填充\"Oxford\"  \n",
    "      iii.如果都不满足，便仅将\"-\"替换为\"空格\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们就完成了对出版日期和出版地的数据清洗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 案例分析三： NYC taxi Data Cleaning  \n",
    "\n",
    "***原数据来源： [NYC Taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyarrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Kris\\\\Github类_自用\\\\dataset\\\\Case 3\\\\yellow_tripdata_2014-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前有: 11830324 列数据\n",
      "目前有: 10501946 列数据 清洗了 1328378 列数据\n"
     ]
    }
   ],
   "source": [
    "dfs = df.shape[0]\n",
    "print(\"目前有:\", dfs,'列数据')\n",
    "\n",
    "df = df[(df[' passenger_count'].between(0,4))]\n",
    "# df = [(df[' passenger_count'] >= 0) & (df[' passenger_count'] <= 4)]\n",
    "\n",
    "print(\"目前有:\", df.shape[0],'列数据',\"清洗了\",dfs-df.shape[0],\"列数据\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df = df[(df[' '].between())]` 是用于筛出特定标签的特定数据范围-在这里用于筛除异常值\n",
    "* 用`.between`会比用大于等于或者小于等于更简洁\n",
    "* 记住不能使用方括号，使用`.between`方法已经是闭区间\n",
    "* 注：纽约的出租车不一定只能载四个人，在这里仅作练习目的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['vendor_id',' pickup_datetime', ' dropoff_datetime',\n",
    "              ' passenger_count',' rate_code', ' store_and_fwd_flag',\n",
    "              ' payment_type',' fare_amount', ' surcharge',\n",
    "              ' mta_tax',' tip_amount',' tolls_amount',' total_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df.shape[0]` 是用于筛出特定标签的特定数据范围-在这里用于筛除异常值\n",
    "* 作为属性，而非方法，不用括号调用\n",
    "* 如果不加`[0]`，能够返回行和列的信息，这里仅需要了解删除了多少数据\n",
    "* 等价于`len(df)`\n",
    "* 区别于：`df.size`，这个属性会输出全部的元素数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前有: 10501946 列数据\n"
     ]
    }
   ],
   "source": [
    "dfs = df.shape[0]\n",
    "print(\"目前有:\", dfs,'列数据')\n",
    "\n",
    "df = df[~((df[' pickup_latitude'] == 0) &\n",
    "          (df[' pickup_longitude'] == 0) &\n",
    "          (df[' dropoff_latitude'] == 0) &\n",
    "          (df[' dropoff_longitude'] == 0))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df = df[~(df['column'] == 0)]` 将满足特定条件标记为Ture，用于后续的的取反筛除\n",
    "* 使用了按位与运算符将同时满足四个条件的行标记为 True，再取反（~）得到要保留的行\n",
    "* &：按位与，用于同时满足多个条件\n",
    "* |：按位或，用于满足任一条件\n",
    "* ^：按位异或，两个布尔值不同返回 True，相同返回 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前有: 10367995 列数据\n",
      "目前有: 10367921 列数据 清洗了 74 列数据\n"
     ]
    }
   ],
   "source": [
    "dfs = df.shape[0]\n",
    "print(\"目前有:\", dfs,'列数据')\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"目前有:\", df.shape[0],'列数据',\"清洗了\",dfs-df.shape[0],\"列数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前有: 10367921 列数据\n",
      "目前有: 10367913 列数据 清洗了 8 列数据\n"
     ]
    }
   ],
   "source": [
    "dfs = df.shape[0]\n",
    "print(\"目前有:\", dfs,'列数据')\n",
    "\n",
    "import math\n",
    "\n",
    "# Haversine 计算地理直线距离（单位：公里）\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # 地球半径（km）\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "# 1. 定义完整数据：pickup 和 dropoff 坐标都不为 0\n",
    "df_full = df[(df[' pickup_latitude'] != 0) & (df[' pickup_longitude'] != 0) &\n",
    "             (df[' dropoff_latitude'] != 0) & (df[' dropoff_longitude'] != 0)].copy()\n",
    "\n",
    "# 计算完整数据中两端的直线距离\n",
    "df_full.loc[:, 'geo_dist'] = haversine(\n",
    "    df_full[' pickup_latitude'], df_full[' pickup_longitude'],\n",
    "    df_full[' dropoff_latitude'], df_full[' dropoff_longitude']\n",
    ")\n",
    "\n",
    "# 计算行驶距离与直线距离的比值，并取中位数作为代表\n",
    "df_full.loc[:, 'ratio'] = df_full[' trip_distance'] / (df_full['geo_dist'] + 1e-5)\n",
    "ratio_mean = df_full['ratio'].median()\n",
    "\n",
    "# 2. 定义缺失数据：\n",
    "#    缺失 pickup：pickup 坐标为 0，但 dropoff 坐标存在\n",
    "df_missing_pickup = df[(df[' pickup_latitude'] == 0) & (df[' pickup_longitude'] == 0) & \n",
    "                       (df[' dropoff_latitude'] != 0) & (df[' dropoff_longitude'] != 0)].copy()\n",
    "\n",
    "#    缺失 dropoff：dropoff 坐标为 0，但 pickup 坐标存在\n",
    "df_missing_dropoff = df[(df[' dropoff_latitude'] == 0) & (df[' dropoff_longitude'] == 0) & \n",
    "                        (df[' pickup_latitude'] != 0) & (df[' pickup_longitude'] != 0)].copy()\n",
    "\n",
    "# 定义在圆上随机生成点的函数：以 (lat, lon) 为圆心，distance_km 为半径随机生成一个点\n",
    "def simulate_point(lat, lon, distance_km):\n",
    "    angle = np.random.uniform(0, 2 * np.pi)\n",
    "    # 简单换算：1° 约等于 111 km\n",
    "    dx = (distance_km / 111) * np.cos(angle)\n",
    "    dy = (distance_km / 111) * np.sin(angle)\n",
    "    return lat + dy, lon + dx\n",
    "\n",
    "# 3. 补全缺失的 pickup 坐标\n",
    "pickup_lat_sim = []\n",
    "pickup_lon_sim = []\n",
    "for i, row in df_missing_pickup.iterrows():\n",
    "    # 根据 trip_distance 和 ratio_mean 估计直线距离（单位 km）\n",
    "    est_geo_dist = row[' trip_distance'] / ratio_mean\n",
    "    # 以 dropoff 坐标为圆心生成随机点\n",
    "    lat, lon = simulate_point(row[' dropoff_latitude'], row[' dropoff_longitude'], est_geo_dist)\n",
    "    pickup_lat_sim.append(lat)\n",
    "    pickup_lon_sim.append(lon)\n",
    "\n",
    "df_missing_pickup.loc[:, ' pickup_latitude'] = pickup_lat_sim\n",
    "df_missing_pickup.loc[:, ' pickup_longitude'] = pickup_lon_sim\n",
    "\n",
    "# 4. 补全缺失的 dropoff 坐标\n",
    "dropoff_lat_sim = []\n",
    "dropoff_lon_sim = []\n",
    "for i, row in df_missing_dropoff.iterrows():\n",
    "    est_geo_dist = row[' trip_distance'] / ratio_mean\n",
    "    # 以 pickup 坐标为圆心生成随机点\n",
    "    lat, lon = simulate_point(row[' pickup_latitude'], row[' pickup_longitude'], est_geo_dist)\n",
    "    dropoff_lat_sim.append(lat)\n",
    "    dropoff_lon_sim.append(lon)\n",
    "\n",
    "df_missing_dropoff.loc[:, ' dropoff_latitude'] = dropoff_lat_sim\n",
    "df_missing_dropoff.loc[:, ' dropoff_longitude'] = dropoff_lon_sim\n",
    "\n",
    "# 5. 合并所有数据：完整数据、补全 pickup 和补全 dropoff 的数据\n",
    "df = pd.concat([df_full, df_missing_pickup, df_missing_dropoff], ignore_index=True)\n",
    "\n",
    "print(\"目前有:\", df.shape[0],'列数据',\"清洗了\",dfs-df.shape[0],\"列数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前有: 10367913 列数据\n",
      "目前有: 10360154 列数据 清洗了 7759 列数据\n"
     ]
    }
   ],
   "source": [
    "dfs = df.shape[0]\n",
    "print(\"目前有:\", dfs,'列数据')\n",
    "\n",
    "df = df[(df[' pickup_latitude'].between(40.4774, 40.9155) & \n",
    "        df[' pickup_longitude'].between(-74.2597, -73.700) &\n",
    "        df[' dropoff_latitude'].between(40.4774, 40.9155) &\n",
    "        df[' dropoff_longitude'].between(-74.2597, -73.700))]\n",
    "\n",
    "print(\"目前有:\", df.shape[0],'列数据',\"清洗了\",dfs-df.shape[0],\"列数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['geo_dist', 'ratio'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install folium\n",
    "import folium\n",
    "from folium.plugins import FastMarkerCluster, HeatMap\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "df_sample = df.head(10000)\n",
    "\n",
    "# 以纽约市中心为地图初始中心\n",
    "ny_center = [40.7128, -74.0060]\n",
    "m = folium.Map(location=ny_center, zoom_start=12)\n",
    "\n",
    "# 构造两个列表，分别存放 pickup 和 dropoff 的经纬度数据\n",
    "pickup_locations = df_sample[[' pickup_latitude', ' pickup_longitude']].values.tolist()\n",
    "dropoff_locations = df_sample[[' dropoff_latitude', ' dropoff_longitude']].values.tolist()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 使用 FastMarkerCluster 聚合大量数据点\n",
    "# -------------------------------\n",
    "FastMarkerCluster(data=pickup_locations, name='Pickup FastCluster').add_to(m)\n",
    "FastMarkerCluster(data=dropoff_locations, name='Dropoff FastCluster').add_to(m)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 添加 HeatMap 热力图展示数据密度\n",
    "# -------------------------------\n",
    "# 这里使用 radius 和 blur 参数调整热力图的视觉效果\n",
    "HeatMap(data=pickup_locations, name='Pickup HeatMap', radius=8, blur=10, min_opacity=0.5).add_to(m)\n",
    "HeatMap(data=dropoff_locations, name='Dropoff HeatMap', radius=8, blur=10, min_opacity=0.5).add_to(m)\n",
    "\n",
    "# 添加图层控制（可选）\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# 保存地图为 HTML 文件，方便在浏览器中查看\n",
    "m.save(\"ny_taxi_distribution.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
